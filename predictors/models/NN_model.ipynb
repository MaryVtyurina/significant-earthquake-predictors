{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils import data\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for dataset preparation\n",
    "Data is joined by 1 hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(ndays):\n",
    "    path_in = '../../ionosphere_dataset/NOAA/NOAA_datasets_for_ML/dataset_NN_model/sondes_in_ml.csv'\n",
    "    df_inside = pd.read_csv(path_in, sep=',')\n",
    "\n",
    "    path_out = '../../ionosphere_dataset/NOAA/NOAA_datasets_for_ML/dataset_NN_model/sondes_year_before_after_ml.csv'\n",
    "    df_outside = pd.read_csv(path_out, sep=',')\n",
    "    print(df_inside.id.unique().shape[0],df_outside.id.unique().shape[0])\n",
    "\n",
    "    df_inside.nday = df_inside.nday.astype('int')\n",
    "    df_inside = df_inside[(df_inside.nday < ndays)]\n",
    "\n",
    "    df_outside.nday = df_outside.nday.astype('int')\n",
    "    df_outside = df_outside[(df_outside.nday < ndays)]\n",
    "\n",
    "    # join data per hour\n",
    "    means = df_inside.groupby(['id', 'date', 'nday', 'h']).mean()\n",
    "    df_inside = means.reset_index()\n",
    "\n",
    "    means = df_outside.groupby(['id', 'date', 'nday', 'h']).mean()\n",
    "    df_outside = means.reset_index()\n",
    "    \n",
    "    df_inside = df_inside.groupby('id').filter(lambda x: x['D'].count() == 24*ndays)\n",
    "    df_inside.reset_index(drop=True, inplace=True)\n",
    "    # dfnew.id.unique().shape\n",
    "\n",
    "    df_outside = df_outside.groupby('id').filter(lambda x: x['D'].count() == 24*ndays)\n",
    "    df_outside.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df_inside['res'] = 1\n",
    "    df_outside['res'] = 0\n",
    "    df_inside.reset_index(drop=True, inplace=True)\n",
    "    df_outside.reset_index(drop=True, inplace=True)\n",
    "    ds = pd.concat([df_inside, df_outside], sort=True)\n",
    "    ds = ds.drop(columns=['m', 'date'])\n",
    "    \n",
    "    print('dataset size:', ds.id.unique().shape, ds.shape)\n",
    "    print('negative samples: %f, positive samples: %f'% (df_inside.id.unique().shape[0], df_outside.id.unique().shape[0]))\n",
    "    return ds\n",
    "    \n",
    "\n",
    "def remove_feature(ds, feature):\n",
    "    return ds.drop(columns = [feature])\n",
    "\n",
    "def ds_to_tensor(ds):\n",
    "    grouped_by_id = ds.groupby(['id', 'nday',])\n",
    "    X = []\n",
    "    y = []\n",
    "    for name, g in grouped_by_id:\n",
    "        X.append(g.drop(columns=['id', 'nday', 'h']).to_numpy())\n",
    "        y.append(g.res.iloc[0])\n",
    "#     print('len(X), len(y)', len(X), len(y))\n",
    "    X = torch.Tensor(X)\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    return X, y\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, 3)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sample = []\n",
    "        for eq in x:\n",
    "            inner_sample = []\n",
    "            for h in eq:\n",
    "                inner_sample.append(self.linear1(h.type(torch.FloatTensor)))\n",
    "            sample.append(torch.cat(inner_sample, dim = 0))\n",
    "        y_pred = self.linear2(torch.stack(sample, dim = 0))\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def split_data(dataset, batch_size):\n",
    "    data_size = dataset.__len__()\n",
    "    validation_split = .2\n",
    "    split = int(np.floor(validation_split * data_size))\n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                             sampler=val_sampler)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs, lr_scheduler = None):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            prediction = model(x) \n",
    "            loss_value = loss(prediction, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "        if lr_scheduler: \n",
    "            lr_scheduler.step()\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        print('correct_samples=', correct_samples, 'total_samples =', total_samples)\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(loss_accum))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        \n",
    "        print(\"loss: %f, Train accuracy: %f, Val accuracy: %f\" % (loss_value, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        batch_pred = model(x).argmax(1)\n",
    "        correct += (batch_pred == y).nonzero().size(0)\n",
    "        total += y.size(0)     \n",
    "    acc = correct / total\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732 1676\n",
      "dataset size: (1868,) (44832, 18)\n",
      "negative samples: 547.000000, positive samples: 1321.000000\n",
      "correct_samples= tensor(879) total_samples = 1495\n",
      "loss: 2.602733, Train accuracy: 0.587960, Val accuracy: 0.313673\n",
      "correct_samples= tensor(835) total_samples = 1495\n",
      "loss: 3.242531, Train accuracy: 0.558528, Val accuracy: 0.710456\n",
      "correct_samples= tensor(883) total_samples = 1495\n",
      "loss: 2.034647, Train accuracy: 0.590635, Val accuracy: 0.436997\n",
      "correct_samples= tensor(867) total_samples = 1495\n",
      "loss: 1.268242, Train accuracy: 0.579933, Val accuracy: 0.670241\n",
      "correct_samples= tensor(894) total_samples = 1495\n",
      "loss: 1.568455, Train accuracy: 0.597993, Val accuracy: 0.710456\n",
      "correct_samples= tensor(912) total_samples = 1495\n",
      "loss: 1.161656, Train accuracy: 0.610033, Val accuracy: 0.471850\n",
      "correct_samples= tensor(879) total_samples = 1495\n",
      "loss: 2.565173, Train accuracy: 0.587960, Val accuracy: 0.568365\n",
      "correct_samples= tensor(909) total_samples = 1495\n",
      "loss: 1.186947, Train accuracy: 0.608027, Val accuracy: 0.654155\n",
      "correct_samples= tensor(919) total_samples = 1495\n",
      "loss: 1.417852, Train accuracy: 0.614716, Val accuracy: 0.528150\n",
      "correct_samples= tensor(902) total_samples = 1495\n",
      "loss: 1.376148, Train accuracy: 0.603344, Val accuracy: 0.613941\n"
     ]
    }
   ],
   "source": [
    "ds = get_data(ndays = 1)\n",
    "X, y = ds_to_tensor(ds)\n",
    "dataset = Dataset(X, y)\n",
    "\n",
    "train_loader, val_loader = split_data(dataset, batch_size = 64)\n",
    "\n",
    "D_in = 15\n",
    "H, D_out = 72, 2\n",
    "my_model = TwoLayerNet(D_in, H, D_out)\n",
    "my_model.type(torch.FloatTensor)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=0.001)\n",
    "# lr_schededuler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "loss_history, train_history, val_history = train_model(my_model, train_loader, val_loader, loss, optimizer, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
