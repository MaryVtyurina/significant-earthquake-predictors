{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import geopy.distance\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas import Timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_earthquake_data_csv(year, mag,  filepath):\n",
    "    \"\"\"\n",
    "        Downloads earthquake data from earthquake.usgs.gov for 1 year with \n",
    "\n",
    "        Arguments:\n",
    "        year - year, data will be collected from 01.01 00:00:00 to 12.12 23:59:59 of this year\n",
    "        mag - lower border of the magnitude of the earthquakes\n",
    "        filepath - the path to the directory in which the dataset will be saved\n",
    "\n",
    "        Returns:\n",
    "        saves the csv file to the dataset and prints an info message\n",
    "    \"\"\"\n",
    "    url = 'https://earthquake.usgs.gov/fdsnws/event/1/query.csv?starttime={year}-01-01%2000%3A00%3A00&endtime={year}-12-24%2023%3A59%3A59&minmagnitude={mag}&orderby=time'.format(year = year, mag = mag)\n",
    "    df = pd.read_csv(url)\n",
    "    df.to_csv(filepath + 'earthquake_{year}.csv'.format(year = year), sep='\\t')\n",
    "    print('Data for the {year} was saved to earthquake_{year}.csv'.format(year = year))\n",
    "    \n",
    "    \n",
    "def get_earthquake_dataset(start_year, end_year, path):\n",
    "    \"\"\"\n",
    "        Downloads earthquake data from earthquake.usgs.gov for a sertain period of time\n",
    "\n",
    "        Arguments:\n",
    "        start_year, end_year - start and end of the time period for the dataset\n",
    "        mag - lower border of the magnitude of the earthquakes\n",
    "        path - the path to the directory in which the dataset will be saved\n",
    "    \"\"\"\n",
    "    for y in range(start_year, end_year+1):\n",
    "        get_earthquake_data_csv(y, 4.5, path)\n",
    "        \n",
    "get_earthquake_dataset(1992, 2021, 'earthquakes_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_cols = pd.read_csv('earthquakes_data/earthquake_1992.csv', sep='\\t').columns\n",
    "print(eq_cols)\n",
    "def eq_join_in_one_file(earthquakes_folder):  \n",
    "    df_res = pd.DataFrame()\n",
    "    earthquakes_list = os.listdir(earthquakes_folder)\n",
    "    for name_e in earthquakes_list: \n",
    "        path = earthquakes_folder +'/'+ name_e\n",
    "        print(path)\n",
    "        df_e = pd.read_csv(path, sep='\\t')\n",
    "        df_e['time'] = pd.to_datetime(df_e['time'])\n",
    "        df_e = df_e.dropna(subset=['time'])\n",
    "        try:\n",
    "            df_e['time'] = [datetime.datetime.strptime(str(d).split('.')[0], \"%Y-%m-%d %H:%M:%S\") for d in df_e['time']]\n",
    "        except ValueError:\n",
    "            print(\"An error occured while converting to datetime\")\n",
    "            \n",
    "        df_e['date'] = [datetime.datetime.date(dt) for dt in df_e['time']]\n",
    "        df_e['time'] = [datetime.datetime.time(dt) for dt in df_e['time']]\n",
    "        df_res = pd.concat([df_res, df_e])\n",
    "    df_res.drop(df_res.columns[[0]], axis = 1, inplace = True)\n",
    "    df_res.to_csv('earthquakes_data.csv', index=False)\n",
    "\n",
    "eq_join_in_one_file('earthquakes_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sondes = {'WK545': [x for x in range(1992, 2004)],\n",
    "          'WK546' : [x for x in range(2001, 2021)]\n",
    "          'AK539' : [x for x in range(1992, 1994)],\n",
    "          'TO535' : [x for x in range(1992, 2003)],\n",
    "          'TO536' : [x for x in range(2001, 2021)],\n",
    "          'YG431' : [x for x in range(1992, 2021)],\n",
    "          'OK426' : [x for x in range(1992, 2021)],\n",
    "          'SY951' : [x for x in range(1996, 2006)]}\n",
    "\n",
    "def get_df_columns():\n",
    "    url_txt = \"http://wdc.nict.go.jp/IONO/observation-history/factor-auto-WK546-2011.sjis.txt\"\n",
    "    df = pd.read_csv(url_txt)\n",
    "    df_columns = df.columns.values.tolist()\n",
    "    return df_columns\n",
    "\n",
    "def get_japan_data_csv(sondes, df_columns, filepath):\n",
    "    \"\"\"\n",
    "        Downloads all automatically scaled data from ionosondes in Japan from http://wdc.nict.go.jp catalog\n",
    "\n",
    "        Arguments:\n",
    "        sondes - dictionary with keys - ionosondes names and values - years with records\n",
    "        df_columns - list of columns with the structure of catalog\n",
    "        filepath - the path to the directory in which the dataset will be saved\n",
    "\n",
    "        Returns:\n",
    "        saves the csv file to the dataset and prints an info message\n",
    "    \"\"\"\n",
    "    url_str_beg = \"http://wdc.nict.go.jp/IONO/observation-history/factor-auto-\"\n",
    "    url_str_end = \".sjis.txt\"\n",
    "    for sonde, years in sondes.items():\n",
    "        df = pd.DataFrame(columns=df_columns)\n",
    "        for year in years:\n",
    "            url = url_str_beg + sonde + \"-\" + str(year) + url_str_end\n",
    "#             print(\"Downloading data from \", url)\n",
    "            df1 = pd.read_csv(url)\n",
    "            df = pd.concat([df, df1])\n",
    "        fname = str.format(\"{path}{sonde}.csv\", path = filepath, sonde = sonde)\n",
    "        df.to_csv(fname)\n",
    "        print('Data for the {sonde} sonde was saved to {sonde}.csv'.format(sonde = sonde))\n",
    "    \n",
    "df_columns = get_df_columns()\n",
    "path = \"sondes_japan_data/\"\n",
    "get_japan_data_csv(sondes, df_columns, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_time(df):\n",
    "    df['time'] = df['#                       fmin  ']\n",
    "    df['time'] = df['time'].apply(lambda x: x.split(':')[0])\n",
    "    df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "    df = df.drop('#                       fmin  ', 1)\n",
    "    return df[df.time.notnull()]\n",
    "\n",
    "def convert_tz(row, tz_old = 'Asia/Tokyo', tz_new = 'UTC'):\n",
    "    stamp = Timestamp(pd.to_datetime(row['time'], errors='ignore'), tz=tz_old)\n",
    "    row['time'] = stamp.tz_convert(tz=tz_new)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning time & converting TZ              \n",
    "def clean_sondes(sondes_folder):\n",
    "    sondes_names = os.listdir(sondes_folder)\n",
    "    print(sondes_names)\n",
    "    for name_s in sondes_names:\n",
    "        if '.csv' not in name_s:\n",
    "            continue\n",
    "        path = sondes_folder +'/'+ name_s\n",
    "        df_s = pd.read_csv(path, sep=',')\n",
    "        df_s = clean_time(df_s) \n",
    "        #convert tz\n",
    "        if \"SY951\" in name_s:\n",
    "            df_s = df_s.apply(convert_tz, tz_old = 'Antarctica/Syowa',  axis=1)\n",
    "        else:\n",
    "            df_s = df_s.apply(convert_tz, axis=1)\n",
    "        df_s['time'] = pd.to_datetime(df_s['time'])\n",
    "        df_s['date'] = [datetime.datetime.date(d) for d in df_s['time']] \n",
    "        df_s.drop(df_s.columns[[0,1]], axis = 1, inplace = True)\n",
    "        df_s.to_csv(path, index=False)\n",
    "        print('saved ', path)\n",
    "        \n",
    "clean_sondes('sondes_japan_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_prep_zone(row, sonde, size = 1):\n",
    "    radius = 10**(0.43*row['mag'])*size\n",
    "    coord_e = (row['latitude'], row['longitude'])\n",
    "    coord_s = sondes[sonde][0]\n",
    "    dist = geopy.distance.geodesic(coord_e, coord_s).km\n",
    "    return pd.Series([row['id'], radius, dist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sondes = {\n",
    "          'WK545': ((45.39, 141.68),[x for x in range(1992, 2004)]),\n",
    "          'WK546' : ((45.16, 141.75),[x for x in range(2001, 2021)]),\n",
    "          'AK539' : ((39.725, 140.053),[x for x in range(1992, 1994)]),\n",
    "          'TO535' : ((35.71, 139.45),[x for x in range(1992, 2003)]),\n",
    "          'TO536' : ((35.71, 139.45),[x for x in range(2001, 2021)]),\n",
    "          'YG431' : ((31.20, 130.62),[x for x in range(1992, 2021)]),\n",
    "          'OK426' : ((26.28, 127.81),[x for x in range(1992, 2021)]),\n",
    "          'SY951' : ((-69.00, 39.59),[x for x in range(1996, 2006)])\n",
    "         }\n",
    "\n",
    "\n",
    "columns_list = ['time','eq_lat','eq_lon','mag','earthquake_id','date','prep_zone','eq_sonde_dist', 'sonde','sonde_lat','sonde_lon']\n",
    "\n",
    "temp_col_list = ['time', 'latitude', 'longitude', 'mag', 'id', 'date', 'prep_zone', 'dist', 'sonde','sonde_lat','sonde_lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['time', 'latitude', 'longitude', 'mag', 'id', 'date', 'prep_zone_in', 'prep_zone_out', 'dist', 'sonde','sonde_lat','sonde_lon']\n",
    "def filter_by_prep_zone(row, sonde, size1 = 1, size2 = 2):\n",
    "    radius1 = 10**(0.43*row['mag'])*size1\n",
    "    radius2 = 10**(0.43*row['mag'])*size2\n",
    "    coord_e = (row['latitude'], row['longitude'])\n",
    "    coord_s = sondes[sonde][0]\n",
    "    dist = geopy.distance.geodesic(coord_e, coord_s).km\n",
    "    return pd.Series([row['id'], radius1, radius2, dist])\n",
    "\n",
    "def walk_sondes(sondes_folder, earthquakes_folder, res_path):\n",
    "    df_res = pd.DataFrame(columns=col_list)\n",
    "    sondes_names = os.listdir(sondes_folder)\n",
    "    print(sondes_names)\n",
    "    for name_s in sondes_names:\n",
    "        if '.csv' not in name_s:\n",
    "            continue\n",
    "        path = sondes_folder +'/'+ name_s\n",
    "        sonde_name = name_s.split('.')[0]\n",
    "        print(path)\n",
    "        df_s = pd.read_csv(path, sep=',')             \n",
    "        df_sonde = walk_earthquakes(df_s, sonde_name, earthquakes_folder)\n",
    "        df_res = pd.concat([df_res, df_sonde])\n",
    "        df_res.drop_duplicates(subset=['id', 'sonde'], inplace=True)\n",
    "        print(df_res.shape)\n",
    "    df_res.to_csv(res_path, index=False)\n",
    "\n",
    "def walk_earthquakes(df_s, sonde_name, earthquakes_file):\n",
    "    df_res = pd.DataFrame(columns=col_list)\n",
    "    print(sonde_name)\n",
    "    df = pd.read_csv(earthquakes_file, sep=',')\n",
    "    #filter by prep zone radius\n",
    "    eq_filtered = df.apply(filter_by_prep_zone, sonde=sonde_name, size1=1, size2=2, axis=1)\n",
    "    eq_filtered.columns = ['id', 'prep_zone_in', 'prep_zone_out', 'dist']\n",
    "    df = df.merge(eq_filtered, on='id')\n",
    "    # sondes in the small circle\n",
    "    df_e = df[df.dist <= df.prep_zone_in]\n",
    "\n",
    "    # sondes in the big circle\n",
    "#     df_btw = df[(df.prep_zone_in <= df.dist) & (df.dist <= df.prep_zone_out)]\n",
    "#     df_e = df_btw\n",
    "\n",
    "    for d in df_e['date']:\n",
    "        if d not in df_s.date.unique(): #checking if sonde was working on the day of the earthquake\n",
    "            continue\n",
    "        temp_e = df_e[df_e.date == d][['time', 'latitude', 'longitude', 'mag', 'id', 'date', 'prep_zone_in', 'prep_zone_out', 'dist']]\n",
    "        temp_s = pd.DataFrame({'sonde': [sonde_name], 'sonde_lat' : [sondes[sonde_name][0][0]], 'sonde_lon': [sondes[sonde_name][0][1]]})\n",
    "        temp_e.reset_index(drop=True, inplace=True)\n",
    "        temp_s = pd.concat([temp_s for i in range(temp_e.shape[0])],axis = 0)          \n",
    "        temp_s.reset_index(drop=True, inplace=True)\n",
    "        temp = pd.concat([temp_e, temp_s], axis=1)\n",
    "        df_res = pd.concat([df_res, temp])\n",
    "    return df_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_sondes('sondes_japan_data', 'earthquakes_data.csv', 'sondes_in_prep_zone_small.csv')\n",
    "walk_sondes('sondes_japan_data', 'earthquakes_data.csv', 'sondes_in_prep_zone_big.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
